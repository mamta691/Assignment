{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1042c8bc-f292-43d2-867b-7f9683a1c358",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "Min-Max scaling is a common technique used in data preprocessing to normalize features of a dataset to a common scale. It involves scaling the values of the features in a dataset to a range between 0 and 1. This is achieved by subtracting the minimum value of the feature from each value in the feature and then dividing the result by the range of the feature, which is the difference between the maximum value and the minimum value.\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original feature value, X_scaled is the scaled value of X, X_min is the minimum value of X, and X_max is the maximum value of X.\n",
    "\n",
    "2, 5, 8, 10, 12\n",
    "\n",
    "To scale these values using Min-Max scaling, we would first calculate the minimum and maximum values of the feature:\n",
    "X_min = 2\n",
    "X_max = 12\n",
    "\n",
    "Next, we would apply the formula to each value in the feature:\n",
    "\n",
    "    X_scaled_1 = (2 - 2) / (12 - 2) = 0\n",
    "\n",
    "    X_scaled_2 = (5 - 2) / (12 - 2) = 0.375\n",
    "\n",
    "    X_scaled_3 = (8 - 2) / (12 - 2) = 0.625\n",
    "\n",
    "    X_scaled_4 = (10 - 2) / (12 - 2) = 0.75\n",
    "\n",
    "    X_scaled_5 = (12 - 2) / (12 - 2) = 1\n",
    "\n",
    "    The resulting scaled values are all between 0 and 1, with 0 representing the minimum value and 1 representing the maximum value of the feature.\n",
    "    Min-Max scaling can help in situations where features have different ranges and can improve the performance of machine learning algorithms that rely on distance calculations or gradient descent optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdc056-6d3d-4711-b6bc-fafd4c23b32a",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique, also known as the L2 normalization, is a feature scaling technique used to normalize the values of a feature to a unit vector. The idea behind this technique is to scale each sample (i.e., row of a dataset) to a length of 1 in the feature space. \n",
    "\n",
    "This is achieved by dividing each feature value by the Euclidean norm of the feature vector.\n",
    "The formula for Unit Vector scaling is given as:\n",
    "\n",
    "    X_scaled = X / ||X||\n",
    "\n",
    "    where X is the original feature vector, ||X|| is the Euclidean norm of the feature vector, and X_scaled is the scaled vector.\n",
    "Compared to Min-Max scaling, Unit Vector scaling does not scale the feature values to a specific range but rather normalizes them to a common scale. This technique can be useful when the magnitude of the features is important, and we want to preserve their direction in the feature space.\n",
    "For example, consider a dataset containing the following values for a feature:\n",
    "2, 5, 8, 10, 12\n",
    "To scale these values using the Unit Vector technique, we would first calculate the Euclidean norm of the feature vector:\n",
    "\n",
    "    ||X|| = sqrt(2^2 + 5^2 + 8^2 + 10^2 + 12^2) = 18.165\n",
    "\n",
    "   Next, we would apply the formula to each value in the feature:\n",
    "\n",
    "    X_scaled_1 = 2 / 18.165 = 0.11\n",
    "\n",
    "    X_scaled_2 = 5 / 18.165 = 0.28\n",
    "\n",
    "    X_scaled_3 = 8 / 18.165 = 0.44\n",
    "\n",
    "    X_scaled_4 = 10 / 18.165 = 0.55\n",
    "\n",
    "    X_scaled_5 = 12 / 18.165 = 0.66\n",
    "\n",
    "    The resulting scaled values are now normalized to a common scale and have a Euclidean norm of 1. This technique can be useful in scenarios such as text classification where the frequency of words is important, and we want to normalize them based on their frequency without affecting their direction in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84246e0-cba4-4ede-b710-419bb5292fec",
   "metadata": {},
   "source": [
    "3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principle Component Analysis (PCA) is a statistical technique used for reducing the dimensionality of a dataset while retaining most of the variability in the data. \n",
    "PCA is a powerful tool for data exploration, visualization, and feature extraction.\n",
    "\n",
    "PCA works by identifying the underlying structure in the data and representing it using a smaller number of variables called principal components. These principal components are linear combinations of the original variables and are orthogonal to each other.\n",
    "The first principal component captures the largest amount of variability in the data, and each subsequent component captures the next largest amount of variability, subject to the constraint of being orthogonal to the previous components.\n",
    "\n",
    "PCA is commonly used in data analysis, computer vision, and machine learning to reduce the dimensionality of high-dimensional datasets. By reducing the number of variables, it can simplify the analysis, speed up algorithms, and make the data more interpretable.\n",
    "\n",
    "For example, suppose you have a dataset containing the measurements of several variables such as height, weight, age, and income of a group of individuals. You can apply PCA to this dataset to identify the most important variables that capture the majority of the variability in the data.\n",
    "You may find that the first principal component is a linear combination of height and weight, while the second principal component is a linear combination of age and income. This allows you to reduce the dataset to just two variables, the first and second principal components, while still retaining most of the variability in the original dataset.\n",
    "In summary, PCA is a powerful technique for reducing the dimensionality of high-dimensional datasets while retaining most of the variability in the data. It is commonly used in data analysis, computer vision, and machine learning to simplify the analysis, speed up algorithms, and make the data more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ac97f-4a5e-4769-9462-fa58c4a6ccdb",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "PCA and feature extraction are closely related concepts. In fact, PCA can be used as a feature extraction technique.\n",
    "\n",
    "Feature extraction is the process of transforming raw data into a set of features that are more meaningful and informative for a specific task, such as classification or clustering. The goal of feature extraction is to reduce the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "PCA can be used for feature extraction by identifying the most important patterns or relationships in the data and representing them as a set of principal components. These principal components can be used as features for subsequent analysis, such as classification or clustering.\n",
    "\n",
    "For example, suppose you have a dataset containing images of handwritten digits. Each image is represented as a matrix of pixels, with each pixel corresponding to a feature. However, the high dimensionality of the data makes it difficult to analyze and classify the images.\n",
    "\n",
    "You can use PCA to extract the most important features from the images. PCA will identify the patterns and relationships between the pixels that are most important for distinguishing between the different digits. The resulting principal components can be used as features for subsequent analysis, such as classification.\n",
    "In this way, PCA can be used for feature extraction to reduce the dimensionality of high-dimensional datasets while retaining the most important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc9012-e4f9-4a2e-971b-00da18a29bb3",
   "metadata": {},
   "source": [
    "Question 5 : You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "Answer :\n",
    "In order to use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, I would follow these steps:\n",
    "Identify the numerical features that need to be scaled. In this case, we have three numerical features: price, rating, and delivery time.\n",
    "\n",
    "Apply the Min-Max scaling method to each feature independently. The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_feature = (feature - min(feature)) / (max(feature) - min(feature))\n",
    "\n",
    "This formula scales each feature to a range of values between 0 and 1, where the minimum value in the original feature is mapped to 0, and the maximum value is mapped to 1.\n",
    "Implement the Min-Max scaling method using a library such as Scikit-learn . Here is an example of how to implement Min-Max scaling using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8536f024-85d2-474a-9cbd-2f7623a0b896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_item</th>\n",
       "      <th>price</th>\n",
       "      <th>delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pizza</td>\n",
       "      <td>500</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burger</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pasta</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noodles</td>\n",
       "      <td>120</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  food_item  price  delivery_time\n",
       "0     pizza    500             30\n",
       "1    burger    100             15\n",
       "2     pasta    150             10\n",
       "3   noodles    120              8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating example data for explaing above\n",
    "import pandas as pd\n",
    "dct = {\n",
    "    'food_item':['pizza','burger','pasta','noodles'],\n",
    "    'price':[500,100,150,120],\n",
    "    'delivery_time':[30,15,10,8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dct)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fb0213-d4ae-4f30-afbb-7aa46faca75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_item</th>\n",
       "      <th>price</th>\n",
       "      <th>delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pizza</td>\n",
       "      <td>500</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burger</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pasta</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noodles</td>\n",
       "      <td>120</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  food_item  price  delivery_time\n",
       "0     pizza    500             30\n",
       "1    burger    100             15\n",
       "2     pasta    150             10\n",
       "3   noodles    120              8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating example data for explaing above\n",
    "import pandas as pd\n",
    "dct = {\n",
    "    'food_item':['pizza','burger','pasta','noodles'],\n",
    "    'price':[500,100,150,120],\n",
    "    'delivery_time':[30,15,10,8]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dct)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbbed5-0665-4f6d-a356-1e91bcaf0730",
   "metadata": {},
   "source": [
    "By applying Min-Max scaling to the numerical features, we ensure that the different features are on a similar scale, which can help improve the performance of machine learning models and recommendation algorithms that use the d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f981f45-23f4-4a97-b0d9-34d49757a734",
   "metadata": {},
   "source": [
    "Question 6 : You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Answer :\n",
    "\n",
    "    PCA (Principal Component Analysis) is a dimensionality reduction technique that is commonly used to reduce the number of features in a dataset while retaining the most important information. In the context of building a model to predict stock prices, we could use PCA to reduce the dimensionality of the dataset by identifying the most significant features that are driving the stock price movement.\n",
    "Here is a step-by-step approach to using PCA for this purpose:\n",
    "Standardize the data: The first step is to standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features have the same scale and helps to improve the performance of PCA.\n",
    "\n",
    "Compute the covariance matrix: Next, we compute the covariance matrix of the standardized data. The covariance matrix represents the relationships between the different features in the dataset.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: We then calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in which the data varies the most, while the eigenvalues represent the magnitude of the variation.\n",
    "\n",
    "Select the principal components: We then select the top k eigenvectors with the highest eigenvalues. These eigenvectors are known as the principal components and represent the most important features in the dataset.\n",
    "\n",
    "Project the data onto the principal components: Finally, we project the original data onto the selected principal components to obtain a new, reduced-dimensional dataset. This dataset can then be used as input to a machine learning algorithm to predict stock prices.\n",
    "\n",
    "Can import PCA library from sklearn.decompose module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b55ade7-0523-4c58-a70d-717652fc29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decompose import PCA\n",
    "# Capture 95% explained variability with PCA module\n",
    "pca = PCA(0.95)\n",
    "X_pca = pca.fit_transform(X)\n",
    "# print the variance ratio explained by each principal component\n",
    "print(\"Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print('\\nTop 5 rows of transformed PCA data :\\n',X_pca[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fff940-8313-422b-840b-568e7b266985",
   "metadata": {},
   "source": [
    "By reducing the dimensionality of the dataset using PCA, we can simplify the problem of predicting stock prices and potentially improve the performance of our model. However, it is important to note that PCA may not always improve the performance of a model and should be evaluated carefully in each specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8794539-9782-47e4-b33b-926dca09b1e2",
   "metadata": {},
   "source": [
    "Question 7 : For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797d8574-2596-49ff-af2e-8c21ab4623f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Create an instance of the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1,1))\n",
    "\n",
    "print(data_scaled.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516413a9-6c95-4a94-9073-e3f9b2c23df9",
   "metadata": {},
   "source": [
    "Question 8 : For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Answer :\n",
    "\n",
    "    The number of principal components to retain in PCA depends on the level of variance we want to preserve in the dataset. In general, we want to retain enough principal components to explain a significant portion of the total variance in the data, while also keeping the number of features as small as possible.\n",
    "To determine how many principal components to retain for the given dataset containing the features height, weight, age, gender, and blood pressure, we would perform the following steps:\n",
    "Standardize the data: We would first standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features have the same scale and helps to improve the performance of PCA.\n",
    "\n",
    "Compute the covariance matrix: Next, we would compute the covariance matrix of the standardized data. The covariance matrix represents the relationships between the different features in the dataset.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: We would then calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in which the data varies the most, while the eigenvalues represent the magnitude of the variation.\n",
    "\n",
    "Select the principal components: We would then select the top k eigenvectors with the highest eigenvalues. These eigenvectors are known as the principal components and represent the most important features in the dataset.\n",
    "\n",
    "Evaluate the explained variance: Finally, we would evaluate the amount of variance explained by each principal component and choose the number of principal components that preserve a significant portion of the total variance in the data.\n",
    "\n",
    "Typically, we would select the number of principal components that can explain at least 80% of the total variance in the data. However, the exact number of principal components to retain may depend on the specific dataset and the problem we are trying to solve.\n",
    "In summary, we would need to perform PCA on the given dataset to determine the optimal number of principal components to retain based on the amount of variance we want to preserve.\n",
    "Below is Example code of How I would Perform PCA on above components :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64a5ab8-89f7-45c9-b7a1-b88e58e4faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Blood Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197.264488</td>\n",
       "      <td>78.301335</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>123.426840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181.909333</td>\n",
       "      <td>71.500660</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>103.227835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.938287</td>\n",
       "      <td>86.183821</td>\n",
       "      <td>46</td>\n",
       "      <td>Male</td>\n",
       "      <td>143.637328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188.764607</td>\n",
       "      <td>84.627080</td>\n",
       "      <td>55</td>\n",
       "      <td>Female</td>\n",
       "      <td>113.360023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.582489</td>\n",
       "      <td>65.257311</td>\n",
       "      <td>63</td>\n",
       "      <td>Female</td>\n",
       "      <td>116.065510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Height     Weight  Age  Gender  Blood Pressure\n",
       "0  197.264488  78.301335   37    Male      123.426840\n",
       "1  181.909333  71.500660   41  Female      103.227835\n",
       "2  172.938287  86.183821   46    Male      143.637328\n",
       "3  188.764607  84.627080   55  Female      113.360023\n",
       "4  165.582489  65.257311   63  Female      116.065510"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating random data with given features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(678)\n",
    "\n",
    "# Generate random data for each feature\n",
    "height = np.random.normal(loc=170, scale=10, size=10000)\n",
    "weight = np.random.normal(loc=70, scale=10, size=10000)\n",
    "age = np.random.randint(18, 65, size=10000)\n",
    "gender = np.random.choice(['Male', 'Female'], size=10000)\n",
    "blood_pressure = np.random.normal(loc=120, scale=10, size=10000)\n",
    "\n",
    "# Combine the data into a Pandas DataFrame\n",
    "data = pd.DataFrame({'Height': height, \n",
    "                     'Weight': weight, \n",
    "                     'Age': age, \n",
    "                     'Gender': gender, \n",
    "                     'Blood Pressure': blood_pressure})\n",
    "\n",
    "# Print the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce15ab06-6190-4916-a2d4-5f958299a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Variables :  ['Gender']\n",
      "Numerical Variables   :  ['Height', 'Weight', 'Age', 'Blood Pressure']\n"
     ]
    }
   ],
   "source": [
    "# Seperating categorical and numerical variables in data \n",
    "cat_cols = list(data.columns[data.dtypes == 'object'])\n",
    "num_cols = list(data.columns[data.dtypes != 'object'])\n",
    "# Print Categorical and Numeric Variables\n",
    "print('Categorical Variables : ',cat_cols)\n",
    "print('Numerical Variables   : ',num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b45c2ce-41b1-4eda-9d0a-23a98c40f846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Blood Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197.264488</td>\n",
       "      <td>78.301335</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>123.426840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181.909333</td>\n",
       "      <td>71.500660</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>103.227835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172.938287</td>\n",
       "      <td>86.183821</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>143.637328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188.764607</td>\n",
       "      <td>84.627080</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>113.360023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.582489</td>\n",
       "      <td>65.257311</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>116.065510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Height     Weight  Age  Gender  Blood Pressure\n",
       "0  197.264488  78.301335   37       1      123.426840\n",
       "1  181.909333  71.500660   41       0      103.227835\n",
       "2  172.938287  86.183821   46       1      143.637328\n",
       "3  188.764607  84.627080   55       0      113.360023\n",
       "4  165.582489  65.257311   63       0      116.065510"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Categotrical variables to Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data[cat_cols[0]]=le.fit_transform(data[cat_cols].values.flatten())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09115a21-dac3-4013-bcb1-801af7c6b477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Blood Pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.763392</td>\n",
       "      <td>0.839863</td>\n",
       "      <td>-0.294147</td>\n",
       "      <td>0.99561</td>\n",
       "      <td>0.348117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.212299</td>\n",
       "      <td>0.157775</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>-1.00441</td>\n",
       "      <td>-1.689637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306094</td>\n",
       "      <td>1.630453</td>\n",
       "      <td>0.375350</td>\n",
       "      <td>0.99561</td>\n",
       "      <td>2.387031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.904781</td>\n",
       "      <td>1.474316</td>\n",
       "      <td>1.044847</td>\n",
       "      <td>-1.00441</td>\n",
       "      <td>-0.667462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.436947</td>\n",
       "      <td>-0.468415</td>\n",
       "      <td>1.639956</td>\n",
       "      <td>-1.00441</td>\n",
       "      <td>-0.394522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Height    Weight       Age   Gender  Blood Pressure\n",
       "0  2.763392  0.839863 -0.294147  0.99561        0.348117\n",
       "1  1.212299  0.157775  0.003407 -1.00441       -1.689637\n",
       "2  0.306094  1.630453  0.375350  0.99561        2.387031\n",
       "3  1.904781  1.474316  1.044847 -1.00441       -0.667462\n",
       "4 -0.436947 -0.468415  1.639956 -1.00441       -0.394522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Applying StandardScaler to entire dataframe\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scaler.fit_transform(data),columns=data.columns)\n",
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6caa82-ccbd-445d-955d-26e994429ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Ratio: [0.20491989 0.20113817 0.20024048]\n",
      "\n",
      "Top 5 rows of transformed PCA data :\n",
      "         PC1       PC2       PC3\n",
      "0  0.179922 -1.464986  0.664593\n",
      "1 -0.536445 -1.423220  1.533594\n",
      "2  2.149224  0.738165 -0.709998\n",
      "3  1.316837 -1.776677  1.784522\n",
      "4  0.597166 -0.539852  0.356913\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA with 3 components\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "X_pca = pd.DataFrame(pca.fit_transform(data_scaled),columns=['PC1','PC2','PC3'])\n",
    "# print the variance ratio explained by each principal component\n",
    "print(\"Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print('\\nTop 5 rows of transformed PCA data :\\n',X_pca.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d03914-afc8-437d-9d40-e23c4da4ae86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
